{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecda0d72-3610-49d8-8d3d-25881877bb05",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import Logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "74f3c8e8-8f15-4b57-9ebd-5c94618a1589",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torchvision as tv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1f276603-2d6c-4fae-848d-8d0f5b74f26f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.autograd.variable import Variable\n",
    "from torchvision import transforms, datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ccf1a1f6-fea6-4e4e-97f3-5e219ea0ee8c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# transforms.Grayscale(num_output_channels=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2b983d24-22c3-4b74-bb39-abc1fab0ef6f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# def mnist_data():\n",
    "#     compose = transforms.Compose(\n",
    "#         [transforms.ToTensor(),\n",
    "#          transforms.Normalize((.5, .5, .5), (.5, .5, .5))\n",
    "#         ])\n",
    "#     out_dir = './dataset'\n",
    "#     return datasets.MNIST(root=out_dir, train=True, transform=compose, download=True)\n",
    "# # Load data\n",
    "# data = mnist_data()\n",
    "# # Create loader with data, so that we can iterate over it\n",
    "# data_loader = torch.utils.data.DataLoader(data, batch_size=100, shuffle=True)\n",
    "# # Num batches\n",
    "# num_batches = len(data_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f73f0429-137f-4098-b1bb-c6647d0d5c78",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def mnist_data():\n",
    "    compose = transforms.Compose(\n",
    "        [transforms.ToTensor(),\n",
    "         transforms.Normalize((.5,), (.5,))\n",
    "        ])\n",
    "    out_dir = './dataset'\n",
    "    return datasets.MNIST(root=out_dir, train=True, transform=compose, download=True)\n",
    "# Load data\n",
    "data = mnist_data()\n",
    "# Create loader with data, so that we can iterate over it\n",
    "data_loader = torch.utils.data.DataLoader(data, batch_size=100, shuffle=True)\n",
    "# Num batches\n",
    "num_batches = len(data_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "47afcfdf-4bf7-49e7-aa1e-827ca8991dbd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "it = iter(data_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "482661f0-6c5e-4edf-8ea8-2e5551134e01",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# images, labels = it.next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c00decac-7665-4de2-970f-806cb5136fa9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ce3f07dc-ed75-404a-8c58-05e18166664f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4d2f5c70-b189-44a5-b674-64f882879d37",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class DiscriminatorNet(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    A three hidden-layer discriminative neural network\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(DiscriminatorNet, self).__init__()\n",
    "        n_features = 784\n",
    "        n_out = 1\n",
    "        \n",
    "        self.hidden0 = nn.Sequential( \n",
    "            nn.Linear(n_features, 1024),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Dropout(0.3)\n",
    "        )\n",
    "        self.hidden1 = nn.Sequential(\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Dropout(0.3)\n",
    "        )\n",
    "        self.hidden2 = nn.Sequential(\n",
    "            nn.Linear(512, 256),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Dropout(0.3)\n",
    "        )\n",
    "        self.out = nn.Sequential(\n",
    "            torch.nn.Linear(256, n_out),\n",
    "            torch.nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.hidden0(x)\n",
    "        x = self.hidden1(x)\n",
    "        x = self.hidden2(x)\n",
    "        x = self.out(x)\n",
    "        return x\n",
    "discriminator = DiscriminatorNet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9eaa7fcd-d285-41b0-973b-00130c8c21bf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def images_to_vectors(images):\n",
    "    # print(images.size())\n",
    "    # image1 = images.reshape()\n",
    "    return images.view(images.size(0), 784)\n",
    "\n",
    "def vectors_to_images(vectors):\n",
    "    return vectors.view(vectors.size(0), 1, 28, 28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "554efc64-cd2c-4efa-97e0-9fd702b1c1f6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class GeneratorNet(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    A three hidden-layer generative neural network\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(GeneratorNet, self).__init__()\n",
    "        n_features = 100\n",
    "        n_out = 784\n",
    "        \n",
    "        self.hidden0 = nn.Sequential(\n",
    "            nn.Linear(n_features, 256),\n",
    "            nn.LeakyReLU(0.2)\n",
    "        )\n",
    "        self.hidden1 = nn.Sequential(            \n",
    "            nn.Linear(256, 512),\n",
    "            nn.LeakyReLU(0.2)\n",
    "        )\n",
    "        self.hidden2 = nn.Sequential(\n",
    "            nn.Linear(512, 1024),\n",
    "            nn.LeakyReLU(0.2)\n",
    "        )\n",
    "        \n",
    "        self.out = nn.Sequential(\n",
    "            nn.Linear(1024, n_out),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.hidden0(x)\n",
    "        x = self.hidden1(x)\n",
    "        x = self.hidden2(x)\n",
    "        x = self.out(x)\n",
    "        return x\n",
    "generator = GeneratorNet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a5605e12-50b6-4620-b94c-77b976de4ed8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def noise(size):\n",
    "    '''\n",
    "    Generates a 1-d vector of gaussian sampled random values\n",
    "    '''\n",
    "    n = Variable(torch.randn(size, 100))\n",
    "    return n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "584bf319-dd98-4d32-a17d-1d779a78d6aa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "d_optimizer = optim.Adam(discriminator.parameters(), lr=0.0002)\n",
    "g_optimizer = optim.Adam(generator.parameters(), lr=0.0002)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ff87058d-cc96-4d4a-a340-ac4a1044f1f1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "loss = nn.BCELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9716291c-a8fc-4e26-98c8-9f83d66dd2f1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def ones_target(size):\n",
    "    '''\n",
    "    Tensor containing ones, with shape = size\n",
    "    '''\n",
    "    data = Variable(torch.ones(size, 1))\n",
    "    return data\n",
    "\n",
    "def zeros_target(size):\n",
    "    '''\n",
    "    Tensor containing zeros, with shape = size\n",
    "    '''\n",
    "    data = Variable(torch.zeros(size, 1))\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2abf6d85-2e00-44d4-8f52-1583775282ba",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_discriminator(optimizer, real_data, fake_data):\n",
    "    N = real_data.size(0)\n",
    "    # Reset gradients\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # 1.1 Train on Real Data\n",
    "    prediction_real = discriminator(real_data)\n",
    "    # Calculate error and backpropagate\n",
    "    error_real = loss(prediction_real, ones_target(N) )\n",
    "    error_real.backward()\n",
    "\n",
    "    # 1.2 Train on Fake Data\n",
    "    prediction_fake = discriminator(fake_data)\n",
    "    # Calculate error and backpropagate\n",
    "    error_fake = loss(prediction_fake, zeros_target(N))\n",
    "    error_fake.backward()\n",
    "    \n",
    "    # 1.3 Update weights with gradients\n",
    "    optimizer.step()\n",
    "    \n",
    "    # Return error and predictions for real and fake inputs\n",
    "    return error_real + error_fake, prediction_real, prediction_fake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bf1f1548-3a87-4b1e-a5ec-df05c5da11f2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_generator(optimizer, fake_data):\n",
    "    N = fake_data.size(0)\n",
    "    # Reset gradients\n",
    "    optimizer.zero_grad()\n",
    "    # Sample noise and generate fake data\n",
    "    prediction = discriminator(fake_data)\n",
    "    # Calculate error and backpropagate\n",
    "    error = loss(prediction, ones_target(N))\n",
    "    error.backward()\n",
    "    # Update weights with gradients\n",
    "    optimizer.step()\n",
    "    # Return error\n",
    "    return error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "662368e1-c9e5-47bf-a390-ba7060b93eff",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "num_test_samples = 16\n",
    "test_noise = noise(num_test_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "47810b4c-bbe5-444b-99e7-5aeeb8d7f78a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create logger instance\n",
    "# logger = Logger(model_name='VGAN', data_name='MNIST')\n",
    "# Total number of epochs to train\n",
    "num_epochs = 5\n",
    "for epoch in range(num_epochs):\n",
    "    for n_batch, (real_batch,_) in enumerate(data_loader):\n",
    "        N = real_batch.size(0)\n",
    "        # 1. Train Discriminator\n",
    "        real_data = Variable(images_to_vectors(real_batch))\n",
    "        # Generate fake data and detach \n",
    "        # (so gradients are not calculated for generator)\n",
    "        fake_data = generator(noise(N)).detach()\n",
    "        # Train D\n",
    "        d_error, d_pred_real, d_pred_fake = \\\n",
    "              train_discriminator(d_optimizer, real_data, fake_data)\n",
    "\n",
    "        # 2. Train Generator\n",
    "        # Generate fake data\n",
    "        fake_data = generator(noise(N))\n",
    "        # Train G\n",
    "        g_error = train_generator(g_optimizer, fake_data)\n",
    "        # Log batch error\n",
    "        # logger.log(d_error, g_error, epoch, n_batch, num_batches)\n",
    "        # Display Progress every few batches\n",
    "        if (n_batch) % 100 == 0: \n",
    "            test_images = vectors_to_images(generator(test_noise))\n",
    "            test_images = test_images.data\n",
    "            # logger.log_images(\n",
    "            #     test_images, num_test_samples, \n",
    "            #     epoch, n_batch, num_batches\n",
    "            # );\n",
    "            # Display status Logs\n",
    "            # logger.display_status(\n",
    "            #     epoch, num_epochs, n_batch, num_batches,\n",
    "            #     d_error, g_error, d_pred_real, d_pred_fake\n",
    "            # )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e8c9b00e-fa95-44fb-be4c-cabddc141354",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7bc2096d-f52c-4583-8508-e10565b9758f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import torchvision.transforms as T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "809c15c8-ee10-4dbd-b189-f0112a741d55",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "transform = T.ToPILImage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "04accaaa-68d7-4e8f-968b-36457768408c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<PIL.Image.Image image mode=L size=28x28 at 0x7FA30D3FCC10>\n"
     ]
    }
   ],
   "source": [
    "for i in test_images:\n",
    "    img = transform(i)\n",
    "    print(img)\n",
    "    break\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8f86ec43-2747-4862-a89b-9d96543ee9c9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAADL0lEQVR4nAXBbWgbZQAA4Lzvex+75C65JE2ypmlD2m6k66ZrZ0Glcz9EiqlOHB3FTVC2MlEnKigICqL4ARM/foiIfyp2ClNcFfyh1E0zrTjarZhmutE2LjG5JL3cNZdccnfvffk8YOLWVK5Oq6NyY7fsGADZcdx2SSPQy1yLwSL41UYjQ4IUdZGZOUzZtsIQFNAqgR4R9EBKYegO6apkxIOUuIz0gUKXa3kB14BWmElqnZTjksyIUidj1k4oQHPjEW1Y2g+CYLjSvwF3z2dMajAmPqbNd9QHlthYhW4SRFLo6CUwUn526GROfXHoofdzpz56QlqnoEZikEwM1v+d+LtG+VLo4JsfXsocQCvfQeaGL71iQDf6w9pAtmaGA/if2UNPPbemfnv1HW9krrU2YUK4zBLr3TStzpSePrtnpbx54a2QoFzOmsYSAQIIDwss26Zdz+TcvSNZ9ubtA9cW7rl8k20/COIssQ1ff+/Iqg/yiZXvT18/N/taqbAZ6S84ALqCbJ9/F2bt7eAn/3Veufjp16v5shVk8pGuDvYplhUJdVVtrGq2xlbXj6X34YtCO+BwYhgW4dl0Q6rZljL/jTrjfjC99Ic/b/kN3cQC4CFl4V1844WF+5P55svXX308/NXUFc1OVOwOiGFf22WtE5eI5xfah1Nb8cxMk6MoZXv6ig5D2OpjCStvuqmi+KSk3XkXecxTFDyDy10esJRDc4bjjXv8ycqRrR9/Pq2LVrRpP/qF5RBeRjX58rDM330on0XPjJo98WqxTZU+79AsNGXbZzJ70/SfD9PrILeJo4urG8Z0aohhZLC/lEAtIqXoRdR9aYOb/Kvnp409BZvABNKhQTYSOju1kz56cm7swplT1Xw9s/U24CHyIChj7+/W1nkkz0Sl2dbkOfEX9pZT6BOpNsKEQUs+zAokfOMzX7D34KAYw7fHF929bqNpgphDfXmUc83gwFo4OrDc1/ILhgcEy16DV6DuB8dRC3F87o7FQi8aNcUdo1drEQ4VtGAIkw7o7xI13DzDZ93f6oyXfCTcsTzmDoS66tznU/wSHfTXqtCQvNVx82NE+XdpGP0PltWNqwVPY7oAAAAASUVORK5CYII=",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ea25e47-b7dd-4060-8197-789bfbd8254c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
